{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar (Week 3)\n",
    "\n",
    "Dataset: https://disk.yandex.ru/d/B-bVBC3_1qQltw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torch_data\n",
    "import torchaudio\n",
    "import tqdm.notebook as tqdm\n",
    "import urllib\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n",
    "# public_key = 'https://disk.yandex.ru/d/B-bVBC3_1qQltw'\n",
    "# final_url = base_url + urllib.parse.urlencode(dict(public_key=public_key))\n",
    "# response = requests.get(final_url)\n",
    "# download_url = response.json()['href']\n",
    "# !wget -O biometry_sem.tar.gz \"{download_url}\"\n",
    "# !tar -xf biometry_sem.tar.gz -C data_sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = 'data_sem'\n",
    "DEVICE = 'cpu'\n",
    "TARGETS = os.path.join(DATA, 'targets.csv')\n",
    "IN_DIM = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch_data.Dataset):\n",
    "    @dataclasses.dataclass\n",
    "    class Item:\n",
    "        feats: torch.tensor\n",
    "        speaker: int\n",
    "        age: int\n",
    "        gender: int\n",
    "        room: int\n",
    "\n",
    "    def __init__(self, transform: nn.Module, speakers=(1, 2, 3), genders=(0, 1)):\n",
    "        targets = pd.read_csv(TARGETS)\n",
    "        self._speakers = {}\n",
    "        self._data = []\n",
    "        for _, row in tqdm.tqdm(targets.iterrows(), total=len(targets)):\n",
    "            if int(row.speaker.split('_')[0]) not in speakers:\n",
    "                continue\n",
    "            if row.gender not in genders:\n",
    "                continue\n",
    "            path = os.path.join(DATA, row.audio)\n",
    "            wav = torchaudio.load(path)[0]\n",
    "            feats = transform(wav)[0]\n",
    "            speaker = self._speakers.get(row.speaker, len(self._speakers))\n",
    "            if speaker not in self._speakers:\n",
    "                self._speakers[row.speaker] = len(self._speakers)\n",
    "            self._data.append(self.Item(feats, speaker, row.age, row.gender, int(row.speaker.split('_')[0])))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "    \n",
    "    def speakers(self):\n",
    "        return len(self._speakers)\n",
    "    \n",
    "def collate_fn(batch: list[Dataset.Item]):\n",
    "    feats = [item.feats for item in batch]\n",
    "    max_len = max([feat.shape[1] for feat in feats])\n",
    "    X = torch.zeros((len(batch), IN_DIM, max_len))\n",
    "    for idx, feat in enumerate(feats):\n",
    "        X[idx,:,:feat.shape[1]] = feat\n",
    "    Y = torch.tensor([item.speaker for item in batch], dtype=torch.long, device=DEVICE)\n",
    "    age = [item.age for item in batch]\n",
    "    gender = [item.gender for item in batch]\n",
    "    room = [item.room for item in batch]\n",
    "    return X.to(DEVICE), Y, age, gender, room"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nazmievairat\\anaconda3\\envs\\python312\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22431d0b37484d309b2620363e81c05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = Dataset(torchaudio.transforms.MFCC(n_mfcc=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "Train simple model for biometrics classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden=32, kernel=7):\n",
    "        super().__init__()\n",
    "        self._body = nn.Sequential(\n",
    "            nn.Conv1d(in_dim, hidden, kernel),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden),\n",
    "            nn.Conv1d(hidden, hidden, kernel),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden),\n",
    "            nn.Conv1d(hidden, hidden, kernel),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden),\n",
    "            nn.Conv1d(hidden, hidden, kernel),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.BatchNorm1d(hidden)\n",
    "        )\n",
    "        self._head = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        emb = self._body(X).squeeze(2)\n",
    "        return self._head(emb), emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, epochs=10, batch_size=256):\n",
    "    opt = optim.Adam(model.parameters())\n",
    "    data = torch_data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    losses = []\n",
    "    accs = []\n",
    "    for _ in range(epochs):\n",
    "        loss_sum = 0\n",
    "        acc_sum = 0\n",
    "        batches = 0\n",
    "        for X, Y, _, _, _ in tqdm.tqdm(data):\n",
    "            logits, _ = model.forward(X)\n",
    "            logits = logits.squeeze()\n",
    "            loss = F.cross_entropy(logits, Y)\n",
    "            with torch.no_grad():\n",
    "                acc = torch.sum(torch.argmax(logits, dim=-1) == Y) / X.shape[0]\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            loss_sum += loss.item()\n",
    "            acc_sum += acc.item()\n",
    "            batches += 1\n",
    "        losses.append(loss_sum / batches)\n",
    "        accs.append(acc_sum / batches)\n",
    "        clear_output()\n",
    "        fig, axis = plt.subplots(1, 2, figsize=(15, 7))\n",
    "        axis[0].plot(losses)\n",
    "        axis[1].plot(accs)\n",
    "        plt.show()\n",
    "        print('Train loss:', losses[-1], 'Accuracy:', accs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0547f8bc2bf244028b174a8d4ff5cf72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "Target 923 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(IN_DIM, dataset\u001b[38;5;241m.\u001b[39mspeakers())\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[72], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, epochs, batch_size)\u001b[0m\n\u001b[0;32m     11\u001b[0m logits, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(X)\n\u001b[0;32m     12\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m---> 13\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     15\u001b[0m     acc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m Y) \u001b[38;5;241m/\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\nazmievairat\\anaconda3\\envs\\python312\\Lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: Target 923 is out of bounds."
     ]
    }
   ],
   "source": [
    "model = Model(IN_DIM, dataset.speakers()).to(DEVICE)\n",
    "train(model, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare embeddings\n",
    "\n",
    "Prepare embeddings of trained model, project to lower dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "ages = []\n",
    "genders = []\n",
    "rooms = []\n",
    "data = torch_data.DataLoader(dataset, batch_size=256, collate_fn=collate_fn)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X, _, age, gender, room in tqdm.tqdm(data):\n",
    "        _, emb = model.forward(X)\n",
    "        embeddings.extend(emb.squeeze().cpu().data.numpy())\n",
    "        ages.extend(age)\n",
    "        genders.extend(gender)\n",
    "        rooms.extend(room)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <YOUR CODE IS HERE>\n",
    "emb2dim = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot different classes\n",
    "\n",
    "Plot embeddings for different classes by age, gender and room conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender\n",
    "for gen in [0, 1]:\n",
    "    cur_emb = [emb for emb, gender in zip(emb2dim, genders) if gender == gen]\n",
    "    print(np.mean(cur_emb, axis=0))\n",
    "    x_limits = np.quantile([x for x, _ in cur_emb], [0.01, 0.99])\n",
    "    y_limits = np.quantile([y for _, y in cur_emb], [0.01, 0.99])\n",
    "    cur_emb = np.array(\n",
    "        [(x, y) for x, y in cur_emb if x_limits[0] <= x <= x_limits[1] and y_limits[0] <= y <= y_limits[1]]\n",
    "    )\n",
    "    indexes = np.random.choice(np.arange(len(cur_emb)), size=500, replace=False)\n",
    "    cur_emb = cur_emb[indexes]\n",
    "    plt.plot([x for x, _ in cur_emb], [y for _, y in cur_emb], '.', label=gen)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
